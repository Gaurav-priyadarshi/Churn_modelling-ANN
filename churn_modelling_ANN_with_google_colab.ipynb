{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "churn modelling ANN  with google colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRA7BnFOYpdS"
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX2-pPstbGXm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktM6FAZ1blls"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/ANN/Churn_Modelling.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxsUVLqpbzUM"
      },
      "source": [
        "X = df.iloc[:,3:13]\n",
        "y = df.iloc[:,-1]\n",
        "#creating dummy variable\n",
        "geography = pd.get_dummies(df['Geography'],drop_first = True)\n",
        "gender = pd.get_dummies(df['Gender'],drop_first = True)\n",
        "X =  pd.concat([X,geography,gender],axis = 1)\n",
        "X = X.drop(['Geography','Gender'],axis = 1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "yNDJ_TDai1Tn",
        "outputId": "eeb8ce51-ad24-4f08-a2fb-90c886320f96"
      },
      "source": [
        "X"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>France</th>\n",
              "      <th>Germany</th>\n",
              "      <th>Spain</th>\n",
              "      <th>Female</th>\n",
              "      <th>Male</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>619</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>608</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>502</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>699</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>850</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>771</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96270.64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>516</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101699.77</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>709</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42085.58</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>772</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92888.52</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>792</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38190.78</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CreditScore  Age  Tenure    Balance  ...  Germany  Spain  Female  Male\n",
              "0             619   42       2       0.00  ...        0      0       1     0\n",
              "1             608   41       1   83807.86  ...        0      1       1     0\n",
              "2             502   42       8  159660.80  ...        0      0       1     0\n",
              "3             699   39       1       0.00  ...        0      0       1     0\n",
              "4             850   43       2  125510.82  ...        0      1       1     0\n",
              "...           ...  ...     ...        ...  ...      ...    ...     ...   ...\n",
              "9995          771   39       5       0.00  ...        0      0       0     1\n",
              "9996          516   35      10   57369.61  ...        0      0       0     1\n",
              "9997          709   36       7       0.00  ...        0      0       1     0\n",
              "9998          772   42       3   75075.31  ...        1      0       0     1\n",
              "9999          792   28       4  130142.79  ...        0      0       1     0\n",
              "\n",
              "[10000 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa52tuTvdjO8"
      },
      "source": [
        "# splittng the dataset and sacling the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIyPkUpOfpkH"
      },
      "source": [
        "# bulinding and training of ANN using keras\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import ReLU,LeakyReLU,ELU,PReLU\n",
        "from keras.layers import Dropout\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3goFMa4gmA_",
        "outputId": "0adc2ccf-68db-4cc8-b224-7742a7826c2a"
      },
      "source": [
        "classifier  = Sequential()\n",
        "## adding the input layers,hidden and output layers\n",
        "\n",
        "classifier.add(Dense(units = 10,input_dim = 11,kernel_initializer='he_uniform',activation='relu'))\n",
        "classifier.add(Dropout(0.3))\n",
        "classifier.add(Dense(units = 20,kernel_initializer='he_uniform',activation='relu'))\n",
        "classifier.add(Dropout(0.3))\n",
        "classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu'))\n",
        "classifier.add(Dense(units = 1,kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
        "classifier.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])\n",
        "model_history = classifier.fit(X_train,y_train,validation_split=0.33,batch_size=20,epochs = 100)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "268/268 [==============================] - 13s 5ms/step - loss: 0.6406 - accuracy: 0.7754 - val_loss: 0.5102 - val_accuracy: 0.7955\n",
            "Epoch 2/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7901 - val_loss: 0.4914 - val_accuracy: 0.7955\n",
            "Epoch 3/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.5029 - accuracy: 0.7911 - val_loss: 0.4791 - val_accuracy: 0.7955\n",
            "Epoch 4/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4912 - accuracy: 0.7901 - val_loss: 0.4729 - val_accuracy: 0.7955\n",
            "Epoch 5/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4749 - accuracy: 0.8004 - val_loss: 0.4676 - val_accuracy: 0.7955\n",
            "Epoch 6/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4766 - accuracy: 0.7935 - val_loss: 0.4582 - val_accuracy: 0.7959\n",
            "Epoch 7/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4727 - accuracy: 0.7908 - val_loss: 0.4523 - val_accuracy: 0.7955\n",
            "Epoch 8/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4515 - accuracy: 0.8032 - val_loss: 0.4505 - val_accuracy: 0.7959\n",
            "Epoch 9/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4474 - accuracy: 0.7995 - val_loss: 0.4460 - val_accuracy: 0.7974\n",
            "Epoch 10/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4517 - accuracy: 0.8004 - val_loss: 0.4460 - val_accuracy: 0.7989\n",
            "Epoch 11/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4407 - accuracy: 0.7953 - val_loss: 0.4432 - val_accuracy: 0.8001\n",
            "Epoch 12/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4399 - accuracy: 0.8040 - val_loss: 0.4404 - val_accuracy: 0.8016\n",
            "Epoch 13/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4418 - accuracy: 0.7977 - val_loss: 0.4368 - val_accuracy: 0.8031\n",
            "Epoch 14/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4402 - accuracy: 0.8023 - val_loss: 0.4426 - val_accuracy: 0.8027\n",
            "Epoch 15/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4439 - accuracy: 0.7965 - val_loss: 0.4323 - val_accuracy: 0.8031\n",
            "Epoch 16/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4309 - accuracy: 0.7961 - val_loss: 0.4375 - val_accuracy: 0.8039\n",
            "Epoch 17/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4290 - accuracy: 0.8030 - val_loss: 0.4325 - val_accuracy: 0.8076\n",
            "Epoch 18/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4084 - accuracy: 0.8131 - val_loss: 0.4299 - val_accuracy: 0.8054\n",
            "Epoch 19/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4236 - accuracy: 0.8079 - val_loss: 0.4307 - val_accuracy: 0.8099\n",
            "Epoch 20/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4253 - accuracy: 0.8044 - val_loss: 0.4245 - val_accuracy: 0.8114\n",
            "Epoch 21/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4299 - accuracy: 0.8066 - val_loss: 0.4229 - val_accuracy: 0.8133\n",
            "Epoch 22/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4279 - accuracy: 0.8080 - val_loss: 0.4176 - val_accuracy: 0.8183\n",
            "Epoch 23/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4090 - accuracy: 0.8161 - val_loss: 0.4189 - val_accuracy: 0.8213\n",
            "Epoch 24/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4031 - accuracy: 0.8191 - val_loss: 0.4163 - val_accuracy: 0.8258\n",
            "Epoch 25/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4131 - accuracy: 0.8122 - val_loss: 0.4029 - val_accuracy: 0.8270\n",
            "Epoch 26/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4192 - accuracy: 0.8125 - val_loss: 0.3968 - val_accuracy: 0.8304\n",
            "Epoch 27/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3802 - accuracy: 0.8379 - val_loss: 0.4092 - val_accuracy: 0.8330\n",
            "Epoch 28/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.4029 - accuracy: 0.8194 - val_loss: 0.3970 - val_accuracy: 0.8379\n",
            "Epoch 29/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3913 - accuracy: 0.8344 - val_loss: 0.3994 - val_accuracy: 0.8379\n",
            "Epoch 30/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.4081 - accuracy: 0.8165 - val_loss: 0.3946 - val_accuracy: 0.8395\n",
            "Epoch 31/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3881 - accuracy: 0.8400 - val_loss: 0.3866 - val_accuracy: 0.8376\n",
            "Epoch 32/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3840 - accuracy: 0.8337 - val_loss: 0.3833 - val_accuracy: 0.8387\n",
            "Epoch 33/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3953 - accuracy: 0.8320 - val_loss: 0.3863 - val_accuracy: 0.8406\n",
            "Epoch 34/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3955 - accuracy: 0.8325 - val_loss: 0.3779 - val_accuracy: 0.8410\n",
            "Epoch 35/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3843 - accuracy: 0.8355 - val_loss: 0.3823 - val_accuracy: 0.8402\n",
            "Epoch 36/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3726 - accuracy: 0.8373 - val_loss: 0.3814 - val_accuracy: 0.8406\n",
            "Epoch 37/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3903 - accuracy: 0.8320 - val_loss: 0.3850 - val_accuracy: 0.8410\n",
            "Epoch 38/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3975 - accuracy: 0.8319 - val_loss: 0.3743 - val_accuracy: 0.8406\n",
            "Epoch 39/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3693 - accuracy: 0.8382 - val_loss: 0.3918 - val_accuracy: 0.8425\n",
            "Epoch 40/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3718 - accuracy: 0.8405 - val_loss: 0.3775 - val_accuracy: 0.8429\n",
            "Epoch 41/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3796 - accuracy: 0.8329 - val_loss: 0.3839 - val_accuracy: 0.8429\n",
            "Epoch 42/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3750 - accuracy: 0.8408 - val_loss: 0.3756 - val_accuracy: 0.8436\n",
            "Epoch 43/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3819 - accuracy: 0.8372 - val_loss: 0.3834 - val_accuracy: 0.8432\n",
            "Epoch 44/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3762 - accuracy: 0.8349 - val_loss: 0.3697 - val_accuracy: 0.8425\n",
            "Epoch 45/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3753 - accuracy: 0.8436 - val_loss: 0.3768 - val_accuracy: 0.8466\n",
            "Epoch 46/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3759 - accuracy: 0.8410 - val_loss: 0.3764 - val_accuracy: 0.8474\n",
            "Epoch 47/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3588 - accuracy: 0.8542 - val_loss: 0.3726 - val_accuracy: 0.8489\n",
            "Epoch 48/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3568 - accuracy: 0.8522 - val_loss: 0.3742 - val_accuracy: 0.8485\n",
            "Epoch 49/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3648 - accuracy: 0.8526 - val_loss: 0.3706 - val_accuracy: 0.8489\n",
            "Epoch 50/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3693 - accuracy: 0.8405 - val_loss: 0.3844 - val_accuracy: 0.8489\n",
            "Epoch 51/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3730 - accuracy: 0.8427 - val_loss: 0.3690 - val_accuracy: 0.8463\n",
            "Epoch 52/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3704 - accuracy: 0.8515 - val_loss: 0.3751 - val_accuracy: 0.8512\n",
            "Epoch 53/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3653 - accuracy: 0.8491 - val_loss: 0.3714 - val_accuracy: 0.8516\n",
            "Epoch 54/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3636 - accuracy: 0.8439 - val_loss: 0.3692 - val_accuracy: 0.8508\n",
            "Epoch 55/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3874 - accuracy: 0.8424 - val_loss: 0.3671 - val_accuracy: 0.8493\n",
            "Epoch 56/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3688 - accuracy: 0.8468 - val_loss: 0.3744 - val_accuracy: 0.8523\n",
            "Epoch 57/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3660 - accuracy: 0.8523 - val_loss: 0.3695 - val_accuracy: 0.8504\n",
            "Epoch 58/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3764 - accuracy: 0.8445 - val_loss: 0.3724 - val_accuracy: 0.8531\n",
            "Epoch 59/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3649 - accuracy: 0.8476 - val_loss: 0.3661 - val_accuracy: 0.8519\n",
            "Epoch 60/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3499 - accuracy: 0.8574 - val_loss: 0.3712 - val_accuracy: 0.8527\n",
            "Epoch 61/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3712 - accuracy: 0.8419 - val_loss: 0.3739 - val_accuracy: 0.8523\n",
            "Epoch 62/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3716 - accuracy: 0.8408 - val_loss: 0.3769 - val_accuracy: 0.8531\n",
            "Epoch 63/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3666 - accuracy: 0.8478 - val_loss: 0.3743 - val_accuracy: 0.8527\n",
            "Epoch 64/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3579 - accuracy: 0.8484 - val_loss: 0.3729 - val_accuracy: 0.8512\n",
            "Epoch 65/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3660 - accuracy: 0.8426 - val_loss: 0.3867 - val_accuracy: 0.8519\n",
            "Epoch 66/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3622 - accuracy: 0.8396 - val_loss: 0.3689 - val_accuracy: 0.8527\n",
            "Epoch 67/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3663 - accuracy: 0.8498 - val_loss: 0.3705 - val_accuracy: 0.8527\n",
            "Epoch 68/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3716 - accuracy: 0.8531 - val_loss: 0.3687 - val_accuracy: 0.8523\n",
            "Epoch 69/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3697 - accuracy: 0.8451 - val_loss: 0.3645 - val_accuracy: 0.8527\n",
            "Epoch 70/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3668 - accuracy: 0.8428 - val_loss: 0.3674 - val_accuracy: 0.8542\n",
            "Epoch 71/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3685 - accuracy: 0.8486 - val_loss: 0.3700 - val_accuracy: 0.8516\n",
            "Epoch 72/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8563 - val_loss: 0.3725 - val_accuracy: 0.8542\n",
            "Epoch 73/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3507 - accuracy: 0.8568 - val_loss: 0.3667 - val_accuracy: 0.8493\n",
            "Epoch 74/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3614 - accuracy: 0.8535 - val_loss: 0.3734 - val_accuracy: 0.8508\n",
            "Epoch 75/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3666 - accuracy: 0.8467 - val_loss: 0.3724 - val_accuracy: 0.8512\n",
            "Epoch 76/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3615 - accuracy: 0.8486 - val_loss: 0.3692 - val_accuracy: 0.8497\n",
            "Epoch 77/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3639 - accuracy: 0.8508 - val_loss: 0.3755 - val_accuracy: 0.8478\n",
            "Epoch 78/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3696 - accuracy: 0.8467 - val_loss: 0.3737 - val_accuracy: 0.8523\n",
            "Epoch 79/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3507 - accuracy: 0.8563 - val_loss: 0.3705 - val_accuracy: 0.8482\n",
            "Epoch 80/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3771 - accuracy: 0.8401 - val_loss: 0.3704 - val_accuracy: 0.8497\n",
            "Epoch 81/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3673 - accuracy: 0.8513 - val_loss: 0.3700 - val_accuracy: 0.8531\n",
            "Epoch 82/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8485 - val_loss: 0.3693 - val_accuracy: 0.8482\n",
            "Epoch 83/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3613 - accuracy: 0.8535 - val_loss: 0.3638 - val_accuracy: 0.8493\n",
            "Epoch 84/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3522 - accuracy: 0.8607 - val_loss: 0.3728 - val_accuracy: 0.8527\n",
            "Epoch 85/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3692 - accuracy: 0.8457 - val_loss: 0.3727 - val_accuracy: 0.8501\n",
            "Epoch 86/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3624 - accuracy: 0.8538 - val_loss: 0.3715 - val_accuracy: 0.8523\n",
            "Epoch 87/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3507 - accuracy: 0.8568 - val_loss: 0.3684 - val_accuracy: 0.8554\n",
            "Epoch 88/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3647 - accuracy: 0.8481 - val_loss: 0.3712 - val_accuracy: 0.8550\n",
            "Epoch 89/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3560 - accuracy: 0.8543 - val_loss: 0.3679 - val_accuracy: 0.8546\n",
            "Epoch 90/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3569 - accuracy: 0.8506 - val_loss: 0.3696 - val_accuracy: 0.8527\n",
            "Epoch 91/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3642 - accuracy: 0.8566 - val_loss: 0.3699 - val_accuracy: 0.8504\n",
            "Epoch 92/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3638 - accuracy: 0.8502 - val_loss: 0.3744 - val_accuracy: 0.8519\n",
            "Epoch 93/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3654 - accuracy: 0.8555 - val_loss: 0.3730 - val_accuracy: 0.8508\n",
            "Epoch 94/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3672 - accuracy: 0.8537 - val_loss: 0.3684 - val_accuracy: 0.8516\n",
            "Epoch 95/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3679 - accuracy: 0.8529 - val_loss: 0.3741 - val_accuracy: 0.8523\n",
            "Epoch 96/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3784 - accuracy: 0.8482 - val_loss: 0.3729 - val_accuracy: 0.8542\n",
            "Epoch 97/100\n",
            "268/268 [==============================] - 1s 4ms/step - loss: 0.3651 - accuracy: 0.8548 - val_loss: 0.3679 - val_accuracy: 0.8519\n",
            "Epoch 98/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3667 - accuracy: 0.8413 - val_loss: 0.3730 - val_accuracy: 0.8535\n",
            "Epoch 99/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3399 - accuracy: 0.8607 - val_loss: 0.3731 - val_accuracy: 0.8497\n",
            "Epoch 100/100\n",
            "268/268 [==============================] - 1s 3ms/step - loss: 0.3501 - accuracy: 0.8607 - val_loss: 0.3709 - val_accuracy: 0.8482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6aDgsVKhAe8"
      },
      "source": [
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC48Fw3Ckpl1"
      },
      "source": [
        "y_pred = (y_pred>0.5)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7vrPbWhk6JJ"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "score = accuracy_score(y_pred,y_test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBsdaA-2lX9d",
        "outputId": "c6bd3e8c-f5c5-4f48-f5d3-850a79c9287e"
      },
      "source": [
        "cm"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1534,   61],\n",
              "       [ 212,  193]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4eaN7NoltVA",
        "outputId": "3900af81-6772-48d1-a798-d638995b36ac"
      },
      "source": [
        "score"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8635"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KxABunzluzf",
        "outputId": "e5aaeb80-3b5d-4aa9-b2ce-bc381af87926"
      },
      "source": [
        "model_history.history['val_accuracy']"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7955319881439209,\n",
              " 0.7955319881439209,\n",
              " 0.7955319881439209,\n",
              " 0.7955319881439209,\n",
              " 0.7955319881439209,\n",
              " 0.795910656452179,\n",
              " 0.7955319881439209,\n",
              " 0.795910656452179,\n",
              " 0.7974252104759216,\n",
              " 0.7989398241043091,\n",
              " 0.8000757098197937,\n",
              " 0.8015903234481812,\n",
              " 0.8031048774719238,\n",
              " 0.8027262687683105,\n",
              " 0.8031048774719238,\n",
              " 0.8038621544837952,\n",
              " 0.8076485991477966,\n",
              " 0.8053767681121826,\n",
              " 0.8099204897880554,\n",
              " 0.8114350438117981,\n",
              " 0.8133282661437988,\n",
              " 0.8182506561279297,\n",
              " 0.8212798237800598,\n",
              " 0.8258235454559326,\n",
              " 0.826959490776062,\n",
              " 0.8303672671318054,\n",
              " 0.8330178260803223,\n",
              " 0.8379401564598083,\n",
              " 0.8379401564598083,\n",
              " 0.8394547700881958,\n",
              " 0.8375615477561951,\n",
              " 0.8386974334716797,\n",
              " 0.8405906558036804,\n",
              " 0.8409693241119385,\n",
              " 0.8402120471000671,\n",
              " 0.8405906558036804,\n",
              " 0.8409693241119385,\n",
              " 0.8405906558036804,\n",
              " 0.8424838781356812,\n",
              " 0.8428625464439392,\n",
              " 0.8428625464439392,\n",
              " 0.8436198234558105,\n",
              " 0.8432412147521973,\n",
              " 0.8424838781356812,\n",
              " 0.8466489911079407,\n",
              " 0.847406268119812,\n",
              " 0.8489208817481995,\n",
              " 0.8485422134399414,\n",
              " 0.8489208817481995,\n",
              " 0.8489208817481995,\n",
              " 0.8462703227996826,\n",
              " 0.8511927127838135,\n",
              " 0.8515713810920715,\n",
              " 0.8508141040802002,\n",
              " 0.8492994904518127,\n",
              " 0.8523286581039429,\n",
              " 0.8504354357719421,\n",
              " 0.8530859351158142,\n",
              " 0.8519499897956848,\n",
              " 0.8527073264122009,\n",
              " 0.8523286581039429,\n",
              " 0.8530859351158142,\n",
              " 0.8527073264122009,\n",
              " 0.8511927127838135,\n",
              " 0.8519499897956848,\n",
              " 0.8527073264122009,\n",
              " 0.8527073264122009,\n",
              " 0.8523286581039429,\n",
              " 0.8527073264122009,\n",
              " 0.8542218804359436,\n",
              " 0.8515713810920715,\n",
              " 0.8542218804359436,\n",
              " 0.8492994904518127,\n",
              " 0.8508141040802002,\n",
              " 0.8511927127838135,\n",
              " 0.8496781587600708,\n",
              " 0.8477849364280701,\n",
              " 0.8523286581039429,\n",
              " 0.8481635451316833,\n",
              " 0.8496781587600708,\n",
              " 0.8530859351158142,\n",
              " 0.8481635451316833,\n",
              " 0.8492994904518127,\n",
              " 0.8527073264122009,\n",
              " 0.8500567674636841,\n",
              " 0.8523286581039429,\n",
              " 0.855357825756073,\n",
              " 0.8549791574478149,\n",
              " 0.8546005487442017,\n",
              " 0.8527073264122009,\n",
              " 0.8504354357719421,\n",
              " 0.8519499897956848,\n",
              " 0.8508141040802002,\n",
              " 0.8515713810920715,\n",
              " 0.8523286581039429,\n",
              " 0.8542218804359436,\n",
              " 0.8519499897956848,\n",
              " 0.8534646034240723,\n",
              " 0.8496781587600708,\n",
              " 0.8481635451316833]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvyfNxNumS9J",
        "outputId": "921dcfea-2c8c-4d4d-c5d4-e95dec9c05fa"
      },
      "source": [
        "model_history.history['loss']"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5820972919464111,\n",
              " 0.514089822769165,\n",
              " 0.49419963359832764,\n",
              " 0.48439326882362366,\n",
              " 0.47304394841194153,\n",
              " 0.4706380367279053,\n",
              " 0.46679627895355225,\n",
              " 0.4595407545566559,\n",
              " 0.4543835520744324,\n",
              " 0.45392298698425293,\n",
              " 0.44542384147644043,\n",
              " 0.4444635510444641,\n",
              " 0.44125422835350037,\n",
              " 0.43591582775115967,\n",
              " 0.4350273013114929,\n",
              " 0.4320249557495117,\n",
              " 0.4319949150085449,\n",
              " 0.42427951097488403,\n",
              " 0.42889147996902466,\n",
              " 0.42467889189720154,\n",
              " 0.42091962695121765,\n",
              " 0.4140230119228363,\n",
              " 0.4153693616390228,\n",
              " 0.4088868796825409,\n",
              " 0.4087907075881958,\n",
              " 0.40318968892097473,\n",
              " 0.40172079205513,\n",
              " 0.39953815937042236,\n",
              " 0.3954283595085144,\n",
              " 0.3945634067058563,\n",
              " 0.3918251097202301,\n",
              " 0.38611268997192383,\n",
              " 0.39444100856781006,\n",
              " 0.38781121373176575,\n",
              " 0.38346225023269653,\n",
              " 0.38216936588287354,\n",
              " 0.3822039067745209,\n",
              " 0.37876948714256287,\n",
              " 0.3778095841407776,\n",
              " 0.3741415739059448,\n",
              " 0.3816377818584442,\n",
              " 0.3719383776187897,\n",
              " 0.37892186641693115,\n",
              " 0.370053231716156,\n",
              " 0.37405917048454285,\n",
              " 0.3771677315235138,\n",
              " 0.36748790740966797,\n",
              " 0.3693835437297821,\n",
              " 0.36386027932167053,\n",
              " 0.37380295991897583,\n",
              " 0.3690442144870758,\n",
              " 0.3704186975955963,\n",
              " 0.3744681775569916,\n",
              " 0.36877524852752686,\n",
              " 0.3726199269294739,\n",
              " 0.37703216075897217,\n",
              " 0.36741918325424194,\n",
              " 0.37378445267677307,\n",
              " 0.36469414830207825,\n",
              " 0.36335551738739014,\n",
              " 0.3720749020576477,\n",
              " 0.36780911684036255,\n",
              " 0.3683212995529175,\n",
              " 0.3670502007007599,\n",
              " 0.37010136246681213,\n",
              " 0.3620762526988983,\n",
              " 0.3707413971424103,\n",
              " 0.3641076683998108,\n",
              " 0.3623945116996765,\n",
              " 0.36710691452026367,\n",
              " 0.3696170449256897,\n",
              " 0.3653378486633301,\n",
              " 0.3643420338630676,\n",
              " 0.36699578166007996,\n",
              " 0.36445334553718567,\n",
              " 0.36342668533325195,\n",
              " 0.36273193359375,\n",
              " 0.3603779077529907,\n",
              " 0.36323797702789307,\n",
              " 0.35953837633132935,\n",
              " 0.36371690034866333,\n",
              " 0.36464735865592957,\n",
              " 0.3608075976371765,\n",
              " 0.3642596900463104,\n",
              " 0.3647526204586029,\n",
              " 0.3613917827606201,\n",
              " 0.35545429587364197,\n",
              " 0.3638535737991333,\n",
              " 0.35634759068489075,\n",
              " 0.3652119040489197,\n",
              " 0.366218239068985,\n",
              " 0.3617497682571411,\n",
              " 0.36408284306526184,\n",
              " 0.36596235632896423,\n",
              " 0.3564879596233368,\n",
              " 0.3647494316101074,\n",
              " 0.36226949095726013,\n",
              " 0.36753249168395996,\n",
              " 0.3593654930591583,\n",
              " 0.36119866371154785]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzhbB6UlmpEC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}